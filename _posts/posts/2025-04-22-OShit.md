---
title:   "OShit: Kernel things"
classes: wide
header:
  teaser: /img/postcover/earth.png
ribbon: green
categories:
  - Projects
toc: true
toc_label: "8 weeks of pure hardcore kernel development"
---

```c
I will not go into implementation details, provide extensive explanations of our
solutions, or share code. This is still an active assignment for the course, and
I want to avoid jeopardizing the learning experience or outcomes for other
students in any way.

If you are interested into knowing more or asking any question feel free to
reach out.
```


> This project was done in collaboration with @archMaster on an elbow to elbow
> type of collaboration, we would sit in university at 09:00 and often clock out
> when kicked out from the university security guards at midnight. A lot of
> technical discussions, null pointers deref and problematic commits.

# Intro

For this course we were asked to work in pairs and "build your OS from scratch".
We had part of the infrastructure provided to us by the teaching staff for
obvious time reasons and we were left to implement the core functionality and
logics of everything an OS needs.
In 7 weeks we went from a simple boot allocator to a multicore scheduler with
CPUs affinity and OOM capabilities.

This is how we built OShit, an os that could be a hit, or... maybe not.
```
          *************************************
          ***   Welcome to OShit OS where   ***
          ***   dreams comes true... but    ***
          ***        nightmares too.        ***
          *************************************

Welcome to the OpenLSD kernel monitor!
Type 'help' for a list of commands.
OShit>                           
```

# TL;DR

In this 7-week project, we built a complete operating system from scratch,
progressing through key OS modules:

**Week 1:** Implemented a boot allocator and a buddy allocator for efficient
memory management with power-of-2 sized blocks and transparent huge pages.

**Week 2:** Created page table manipulation code for virtual memory with KASLR
and 2MB page support.

**Week 3:** Added user-space process support, interrupt handling, syscalls, and
a SLAB allocator for small kernel data structures.

**Week 4:** Developed demand paging with virtual memory areas (VMAs),
mmap/munmap syscalls, and a blazingly fast vDSO implementation.

**Week 5:** Built a preemptive multitasking system with fork, wait, exec and
timekeeping, featuring copy-on-write semantics and zero-page deduplication.

**Week 6:** Implemented multi-core support with fine-grained locking, per-CPU
run queues, work stealing, and CPU affinity to efficiently utilize all available
cores.

**Week 7:** Added swap capability with an LRU page reclaiming algorithm, OOM
killer, and asynchronous disk I/O to handle memory pressure gracefully.

Starting with just a simple boot allocator, we progressively built every major
subsystem of a modern OS with competitive performance, solid security, and
intelligent resource management. Each week introduced new challenges that built
on previous solutions, creating a cohesive system that could efficiently manage
hardware resources while providing the illusion of process isolation and
unlimited memory.


# Week 1 - Boot allocator and Buddy allocator

At the base of an operating system there's the `boot allocator`, used to setup
the initial memory, and the lowest possible level of memory allocation, in our
case the buddy page frame allocator, which is used to hand out memory.

The boot allocator is the simplest form of allocation and does not support
memory freeing, therefore is only used in the boot phase to setup the memory for
the buddy allocator and get it running.

The buddy allocator, on the other hand, supports allocation and deallocation
requiring a, even if simple, list that somehow tracks the blocks and their
state.

## Understanding the Buddy Allocator

The buddy allocation system is an efficient (and funny) memory management
technique that minimizes external fragmentation. It works by dividing memory
into power-of-two sized blocks (2^N pages), where each block can be recursively
split into two equal "buddy" blocks when a smaller allocation is needed.

The "order" of a page refers to the exponent in the power-of-two calculation -
for example, an order-0 allocation gives you 2^0 = 1 page, an order-3 allocation
gives you 2^3 = 8 consecutive pages, and so on. This organization makes both
allocation and freeing operations quite efficient:

1. When allocating: If we don't have a block of the requested size, we split a
   larger block in half repeatedly until we get the right size.

2. When freeing: We check if the buddy of our freed block is also free. If it
   is, we merge them back together into a larger block, and recursively continue
   this process up the chain.

The beauty of this system is that finding a buddy is a simple bit-flip operation
on the address - elegant and computationally cheap.
```
addr = 0x1000
order = 0x9
buddy_addr = addr ^ ( 1 << order)
// 0x1200

```
> These crazy math people, gotta love them.

This makes the buddy allocator significantly faster than more complex schemes,
while still providing good fragmentation prevention.

Compared to simpler contiguous allocation schemes, the buddy system it's more
flexible than fixed-size allocators, more efficient than general-purpose
allocators, and much simpler to implement than sophisticated schemes like slab
allocators (which we'd typically build on top of the buddy system later).

## Bonuses

Implementation of transparent huge pages, which are simply pages of order 9 (512
consecutive normal pages). This is pretty easy to do - you just need to search
for a page of order 9 in your list. If present, return it; otherwise fail.
Nothing crazy. This is easily achievable using the lookup function which will
take care of finding a block of order 9 or creating one by splitting bigger
blocks if none of the right order is present (but bigger ones are).

On top of this, we added some simple security mechanisms to protect against
common memory bugs:
- Out-of-bound writes
- Use-after-free (UAF)
- Double free
- Invalid free

Most of these are intuitive enough - simple if statements with pageinfo
parameter checks, with the exception of OOB writes. For those, we implemented
page guards: every page allocation of order N will lead to (2^N) + 2 pages
allocated. The 2 additional pages serve as guards, one on the right and one on
the left. They have a byte marking them as guard pages, and every write will
perform a check - if it's a guard page, it will trigger an OOB write error.

This approach isn't the most efficient, but due to the momentary absence of
virtual memory, there's no better way to implement it. It's a relative problem
since the overhead is only applied in the boot phase. Once virtual memory gets
added (in the next lab) and fired up, that mechanism will take over protection
duties.


# Week 2 - Page Table Management

After implementing our buddy allocator, we moved on to page table management for
virtual memory support in our kernel.

The x86-64 architecture uses a 4-level page table structure to translate virtual
addresses to physical ones. Each virtual address is decomposed into different
segments: bits 39-47 select the PML4 entry, bits 30-38 select the PDPT entry,
bits 21-29 for PD entry, and bits 12-20 for PT entry. The final 12 bits
represent the offset within the 4KB page.

> God's only knows how @archMaster managed to understand me when I used a random
> level name for refering to whatever level, so much brain confusion with these
> similar acronyms, everything was called PDPT and we understood eachother.


We implemented a page walker function that recursively traverses this tree
structure. The page walker accepts callback functions to be executed at each
level, making it extremely flexible. This design allowed us to implement
operations like mapping, unmapping, and querying addresses by simply providing
different callbacks to the same walker code.

It's cool and all but debugging it it's pain, wasted 2 days just because
```c
// I had
next = curr + page_size;
// instead of
next = sign_ext(curr + page_size);
```
I forgot a sign extension so my loop would go from 0x0 to 0xffffffffffffffff
(2^16) instead of 0xffffffffffff (2^12), which is **SIGNIFICANTLY** higher.

Once created the initial page table (PML4) an extra step is necessary to make it
the "in use" one, loading it into the **CR3** register. Once done that we can
proceed to map the rest of the memory that will be added to our main PML4, the
kernel one.

For permissions, we set the NX (no-execute) bit on data pages and ensured proper
user/supervisor bits were set. All pages above USER_LIM were marked as
supervisor-only, preventing user-space processes from accessing kernel memory.


## Bonuses

We extended our page table code to support **2MB pages** (rather than only 4KB
pages). The system automatically merges consecutive 4KB pages with identical
permissions into 2MB huge pages when possible and splits them when needed. This
was accomplished through `merge` and `split` functions, which handle the
conversion between page sizes transparently to the rest of the code. The huge
page support significantly reduces TLB pressure for large memory regions.

We implemented kernel address space layout randomization (**KASLR**) randomizing the
location of .text and subsequent sections to make it harder for the attackers to
predict addresses.

# Week 3 - User Tasks & Interrupts

After we set up virtual memory in Lab 2, it was time to add proper process
support, memory allocation for small objects, and interrupt handling. This was a
pivotal lab where our OS truly started to become multi-ring with proper
isolation between kernel and user space... even cause we didn't had any user
space before that, lol.

## SLAB Allocator: When 4KB Is Too Much

While our buddy allocator from Lab 1 worked well for page-sized allocations, we
needed something more efficient for small allocations like kernel data
structures. The solution was implementing a SLAB allocator on top of our
existing buddy allocator.

The SLAB allocator divides individual pages into fixed-size "objects" that can
be allocated and freed independently. The two core functions:

1. `slab_alloc_chunk()`: Allocates a new page from the buddy allocator, sets up
   a `struct slab_info` at the end of the page, and initializes a linked list of
   free objects within the page.

2. `slab_free_chunk()`: Returns a page to the buddy allocator once all objects
   in the SLAB are freed.

This gave us a proper `kmalloc()`/`kfree()` interface for smaller allocations,
used for managing process metadata without wasting memory.

## User Tasks: Breaking Out of Ring 0

Creating user processes required building several components:

1. **Task Management**: We have a `struct task` to track process state,
   address space, and register values. Each task gets a unique PID, stored in
   the global `tasks` array.

2. **Memory Mapping Functions**: To support loading and running ELF binaries, we
   need to be able to map memory for a region without freeing existing pages,
   adjust page permission for a specific range and allocate and initialize a
   PML4 for the process address space.

3. **ELF Loading**: Since we don't have a filesystem yet, we embedded user
   binaries in the kernel image and implemented `task_load_elf()` to parse ELF
   headers and map the appropriate memory regions.

4. **Process Creation & Execution**: We built `task_create()` and `task_run()`
   to set up and switch to user processes.

Once loaded the process and initialized all its required memory we were finally
able to execute the first user program:
```c
#include <lib.h>

/* Trigger a division by zero exception. */
int main(int argc, char **argv)
{
  asm("mov    $0x1, %rax\n\t"
      "mov    $0x0, %rcx\n\t"
      "cltd\n\t"
      "idiv %rcx" );
}
```

Yeah, it's a division by 0 in `asm`. Do you know what happens when you do that?
The CPU raises an exception, number `0`, which would usually be handled by the
interrupt handler... not implemented yet by us... so we get a triple fault.

**First fault:** The CPU tries to handle it using the IDT (Interrupt Descriptor
Table), looking up vector 0. 

**Second fault:** Fault handling fails Then the CPU fails while trying to handle the
first fault â€” so it raises a second fault, like a #GP (General Protection Fault)
or #PF (Page Fault).
    
**Third fault:** Double fault handler also fails The CPU then invokes the double
fault handler (vector 8). Which also fails, so the CPU gives up entirely and
issues a triple fault.

On x86 CPUs, a triple fault causes the CPU to reset itself, just as if the
system had been rebooted via hardware reset.

> First time seeing our - soon to be - best friends GPs and PFs errors.

## Interrupts and Exceptions: Handling Faults

The final piece was implementing proper interrupt handling in x86-64. We created
handlers for CPU interrupts 0-31 (and later 128 for syscalls) in assembly using
a macro.

We then set up entries in the Interrupt Descriptor Table (IDT) to point to these
handlers. For page faults, we implemented code to read the CR2 register
(containing the faulting address) and added special handling to panic if a fault
originated in kernel mode.

For breakpoints (int3 instruction), we implemented special handling to drop into
the kernel monitor for debugging.

## System Calls: Talking to the Kernel

Finally, we implemented a system call interface to allow user processes to
request kernel services (like printing to the screen). We used interrupt 0x80 as
our syscall vector and implemented validation checks to ensure user processes
couldn't pass invalid pointers to the kernel.

Once completed, the `hello` program could properly print "Hello, world!" to the
console through syscalls instead of directly accessing the hardware.
```c
#include <lib.h>

int main(int argc, char **argv)
{
	/* Hello, world! */
	printf("Hello, world!\n");

	printf("We are number %u!\n", getpid());
}
```
```
[PID     0] New task with PID 1
Hello, world!
We are number 1!
[PID     1] Exiting gracefully
[PID     1] Freed task with PID 1
Destroyed the only task - nothing more to do!
OShit> 
```

## Bonuses

We implemented multiple security bonuses:

1. **SYSCALL/SYSRET**: Instead of using slower interrupts, we implemented the
   modern `syscall`/`sysret` instructions for faster system calls. This required
   setting up Model Specific Registers (MSRs) and writing specialized assembly
   handlers that properly swap between kernel and user stacks.

2. **SMEP & SMAP**: We enabled Supervisor Mode Execution/Access Prevention to
   prevent the kernel from accidentally executing or accessing user memory
   without explicit intent. This required manipulating CR4 control registers and
   adding `stac`/`clac` instructions in specific code paths where the kernel
   needs to intentionally access user memory.

3. **Microarchitectural Attack Mitigations**: We added protections against
   Foreshadow and RIDL side-channel attacks through PTE inversion (clearing page
   table entries when freeing memory) and the VERW instruction in asm to flush
   speculative execution buffers.

4. **ASLR**: We implemented address space layout randomization for user
   processes by adding a random offset when loading ELF binaries. This required
   modifying the binary loading code to adjust all virtual addresses
   consistently.

With all these features in place, our OS could now properly run user processes
in ring 3, handle exceptions, provide system call services, and incorporate
modern security practices. The critical foundation for a secure, multi-process
operating system was complete.

# Week 4 - VMAs & Demand Paging

With the bare essentials of user mode and interrupts set up in Lab 3, we moved
on to implementing a more sophisticated memory management system. In modern
operating systems, when a process requests memory, the OS doesn't immediately
allocate physical pages - it simply promises them. This approach, called demand
paging, significantly improves memory efficiency.

We implemented **Virtual Memory Areas** to track what memory regions a process
thinks it owns, even if those regions don't yet have physical backing. Each VMA
represents a contiguous section of virtual memory with the same access
permissions and attributes.

For efficient lookups, each task now maintains two data structures:
1. A red-black tree (`task_rb`) for quickly finding the VMA containing a
   specific address
2. A sorted linked list (`task_mmap`) for easily traversing VMAs in address
   order

Building this foundation required implementing functions for:
- `add_anonymous_vma()`: For regions like the heap or stack
- `add_executable_vma()`: For program code segments
- `insert_vma()`: To insert VMAs into both data structures
- `remove_vma()`: To cleanly remove VMAs

## Demand Paging

The heart of this lab was implementing demand paging. Previously, our
`task_load_elf()` function would immediately map all program segments and the
stack into memory. Now, it simply creates VMAs describing these regions.

When a process tries to access an unmapped page, a page fault occurs. We
implemented a fault handler to:
1. Check if the fault address is within a valid VMA
2. If so, allocate a physical page and map it
3. If needed, load data from the original program binary
4. Resume the process, which can now access that page

This approach allows processes to use large virtual address spaces while
consuming physical memory only for the pages they actually touch. The savings
are substantial, especially for programs that allocate large buffers but use
only parts of them.

To keep our memory management efficient, we implemented VMA merging and
splitting:

- `merge_vma()`: When adjacent VMAs have identical permissions and attributes,
  we merge them into a single VMA
- `split_vma()`: When a new mapping or unmapping operation affects only part of
  a VMA, we split it into multiple VMAs

These operations keep the number of VMAs minimal while maintaining proper
tracking of memory regions.


## mmap() and munmap()

We expanded our syscall interface to include:

1. `sys_mmap()`: Allows processes to request new memory regions with specific
   attributes
   - Validates addresses to prevent mapping kernel space
   - Supports flags like MAP_ANONYMOUS, MAP_PRIVATE, and MAP_FIXED
   - Handles protection flags (read/write/execute)
   - Optionally pre-populates pages with MAP_POPULATE

2. `sys_munmap()`: Allows processes to release memory regions
   - Removes VMAs in the specified range
   - Frees any physical pages that were allocated


## Bonuses

We liked doing bonuses and for this lab they were trivial so we did all of them:

1. **mprotect() and madvise()**:
   - `mprotect()` changes the protection flags of existing memory regions
   - `madvise()` provides hints about memory usage patterns:
     - MADV_WILLNEED pre-populates pages
     - MADV_DONTNEED removes backing pages (unless dirty)

2. **Transparent Huge Pages**: Building on our huge page support from Lab 2, we
   extended our VMA system to use 2MB pages when appropriate. This required
   checking both size and alignment of memory regions and setting the HUGE_PAGE
   flag correctly.

3. **vDSO (Virtual Dynamic Shared Object)**: We implemented a vDSO mechanism
   allowing certain system calls to be executed without trapping into the
   kernel:
   
   ```
   Time for getpid(): 812 ms for 500000 iterations
   Time for vdso_getpid(): 6 ms for 500000 iterations
   
   Our vDSO is 13433% faster than the syscall.
   ```
   
   The implementation maps a single shared page between kernel and user space,
   containing data structures and code accessible to user processes. The kernel
   updates this page with current data, and user programs can read it directly
   without costly context switches.

With all these features in place, our OS now handles memory far more
efficiently. Programs can allocate gigabytes of virtual memory while only
consuming the physical pages they actually use. The vDSO implementation
dramatically speeds up frequent system calls, and VMAs provide a clean
abstraction for tracking memory regions.


# Week 5 - Multitasking & Process Management

After implementing demand paging in Lab 4, our next major step was adding true
multitasking capabilities to our operating system. This meant implementing a
scheduler, process forking, and preemption - core features of any modern OS.

We started by implementing a simple round-robin scheduler.
The idea is straightforward - each process gets a turn to run, and when it
yields (or gets preempted), the next process in line gets a chance. We created a
run queue (`runq`) as a linked list of tasks ready to execute.

The scheduler was implemented with a `sched_yield()` function that:
1. Selects the next task from the run queue
2. If no tasks are ready, either halts the CPU or drops into the kernel monitor
3. Updates scheduler state and runs the selected task

## Fork: Creating New Processes

`fork()` - the system call that allows a process to create a copy of itself.
Our implementation had several key components:

1. **Process Duplication**: We created `task_clone()` to duplicate a task's
   state, including:
   - Register state (copied from parent)
   - Page tables and address space setup
   - Task control structures

2. **Copy-on-Write (COW)**: Instead of immediately duplicating all memory pages
   (which would be wasteful), we implemented COW semantics:
   - Mark all pages as read-only in both parent and child
   - When either tries to write, the page fault handler creates a copy
   - This dramatically improves fork performance, especially for fork-exec
     patterns

3. **Process Hierarchy**: We maintained proper parent-child relationships:
   - Each task has a linked list of children
   - The child gets a new PID but inherits the parent's PID
   - Special return value handling (0 for child, child's PID for parent)

This allowed programs to create multiple child processes that could run
concurrently.

## Wait: Handling Process Lifecycle

To prevent zombie processes and allow parents to monitor their children, we
implemented `sys_wait()` and `sys_waitpid()`. These system calls let a parent
process:

1. Check if any child processes have terminated
2. Reap zombie processes (free their resources)
3. Block until a specific child terminates
4. Retrieve the exit status of terminated children

When a child process terminates, it becomes a zombie until its parent calls
`wait()`. If the parent has already called `wait()`, it gets unblocked and can
continue execution.

## Preemption: Time-Slicing with the LAPIC Timer

Until this point, our multitasking was cooperative - processes ran until they
explicitly yielded. To handle non-cooperative processes (like `evilchild.c`,
which runs an infinite loop), we implemented preemption using timer interrupts.

We configured the Local APIC (LAPIC) timer to generate periodic interrupts:
1. Added an interrupt handler for IRQ_TIMER
2. Sent an End-of-Interrupt (EOI) signal
3. Called `sched_yield()` from the timer handler

This meant even uncooperative processes would eventually get interrupted,
allowing other processes to run. We later improved our scheduler with time
slicing - each process got a specific time budget, and we kept track of how much
CPU time each process used via the Time Stamp Counter (TSC).

## Bonuses

We implemented multiple bonus features:

1. **Exec System Call**:
   - Created a new `sys_exec(char *binary_name)` system call
   - Unmapped the current process's address space
   - Loaded the new binary in its place
   - This allowed a process to completely replace its code without forking

2. **Zero-Page Deduplication**:
   - Created a single shared zero page in the kernel
   - When a process accessed a newly-mapped anonymous page, we mapped it to this
     shared zero page
   - Used COW mechanisms when the process tried to write to it
   - This significantly reduced memory usage for programs that allocate large
     zero-initialized buffers

3. **Time Keeping**:
   - Implemented system calls like `time()`, `gettimeofday()`, and
     `clock_gettime()`
   - Added `sleep()` and related functions to pause processes for specific
     durations
   - Calibrated the LAPIC timer using the HPET for accurate timing
   - Created a global time counter updated on each timer tick
   - Used a linked list of sleeping processes sorted by wake-up time

With these features in place, our OS could now handle multiple processes
concurrently, manage their lifecycle, provide accurate timing, and efficiently
use memory through COW and zero-page deduplication.


# Week 6 - Multi-Core Support

In our final (official) lab, we implemented one of the most significant
challenges in operating system design: multi-processor support. While our
previous labs had built a fully functional single-core OS, modern hardware
demands efficient use of multiple CPU cores, cause who wants to wait?
This required redesigning core components with concurrency in mind.

## Booting Additional Cores

The x86 architecture uses an asymmetric multi-processing model where one CPU
core (the BSP - Bootstrap Processor) starts first and then initializes the other
cores (APs - Application Processors). We implemented `mem_init_mp()` to:

1. Allocate kernel stacks for each additional CPU core
2. Set up per-CPU data structures in memory
3. Initialize memory-related per-CPU state

Once this foundation was laid, we could call `boot_cpus()` to start the
additional cores, which executed their own initialization sequence.

## Synchronization Approaches

### 1. The Big Kernel Lock (BKL)

The first solution was the simplest - a global "big kernel lock" that only one
CPU could hold at a time. This approach is similar to what early versions of
Linux used:

- Each CPU acquires the lock when entering kernel mode
- The lock is released when returning to user mode
- While simple, this effectively serializes kernel execution

This approach worked but severely limited potential performance gains from
multiple cores. Only one core could execute kernel code at any time, creating a
significant bottleneck.

### 2. Fine-Grained Locking

We replaced the BKL with a more sophisticated system using multiple locks for
different resources:

1. **Buddy Allocator Locks**: Protected the physical memory allocator data
   structures
2. **Console Locks**: Ensured atomic console output
3. **Task-Specific Locks**: Protected individual task data structures

The most complex change was to the scheduler:

> AAAAAAAAAAAAAAAAAA THE PAIN AAAAAAAAAAAAAAAAAA

- Each CPU now had its own local run queue without locks
- A global run queue was protected by a spinlock
- Tasks were periodically migrated between local and global queues
- When a local queue was depleted, a CPU could steal work from the global queue

This approach allowed true parallelism in the kernel while ensuring data
structure integrity.

### 3. Atomic Operations

For simple counters and flags that needed atomic updates, we leveraged hardware
atomic instructions through a provided interface. These operations
(like atomic increment) are executed without interruption, eliminating the need
for locks in simple cases.

## Kernel Threads

We extended our system to support kernel threads by adding a new task type:
`TASK_TYPE_KERNEL`. Unlike user processes, kernel threads:

- Run entirely in kernel space
- Don't have user address spaces or page tables
- Execute with supervisor privileges
- Can access kernel memory directly

We implemented a "zero page" kernel thread that runs at low priority, constantly
looking for freed pages that need zeroing - similar to Windows' zero page
thread. This improved performance by ensuring pages were ready for immediate use
when allocated.

## Bonuses

We implemented all three bonus options, which worked together to create a highly
efficient multi-core system:

**Core Hotplugging**

Cores could be dynamically enabled and disabled via new system calls:
- `core_disable(int8_t cpu_id)`
- `core_enable(int8_t cpu_id)`

When there wasn't enough work, unused cores would halt themselves (saving power)
and periodically wake to check for new work. When a core was explicitly
disabled, all its tasks would be migrated to other cores.

**CPU Affinity**

We added CPU affinity support, allowing processes to be "pinned" to specific
cores:
- `sched_setaffinity(pid_t pid, int8_t cpu_id)`
- `sched_getaffinity(pid_t pid)`

This was particularly useful for applications with specific performance needs or
cache behavior. Our implementation treated affinity as a strong suggestion but
would still run tasks on other cores if necessary to prevent starvation.

**Efficient Multi-Core Frame Allocator**

We significantly optimized our buddy allocator for multi-core operation by:
- Creating per-CPU free lists to reduce contention
- Implementing a dedicated kernel thread (`kbuddy_allocator`) that pre-allocates
  pools of free pages for each CPU
- Enabling fast, lock-free allocation from the per-CPU pools
- Falling back to the global allocator (with locking) only when local pools were
  depleted

This approach drastically reduced lock contention, which had been a significant
bottleneck in earlier implementations.

# Week 7 - Memory Pressure & Swap

> This lab was not mandatory but we truly enjoyed the course so we gladly did it 

For our (REAL) final lab, we implemented one of the most crucial components of
any modern operating system: virtual memory swapping. While our OS had already
mastered demand paging and multi-core processing, it still couldn't handle
situations where physical memory was exhausted. The final piece of the puzzle
was implementing a swap system that could move infrequently used pages to disk
to free physical memory.

Modern operating systems give applications the illusion of having vast amounts
of memory, significantly more than what's physically available. This is achieved
by using disk space as an extension of RAM - a technique known as swapping. When
physical memory becomes scarce, the kernel must make decisions about which pages
to keep in memory and which to temporarily move to disk.

We implemented several core mechanisms to handle memory pressure:

**Out-of-Memory (OOM) Killer**

As a last resort when memory is critically low, the system needs a way to
reclaim memory quickly. We implemented an OOM killer that:

- Runs as a kernel thread, periodically checking memory usage against a high
  watermark
- When memory usage exceeds this threshold, it scans all running tasks
- Identifies processes with the largest Resident Set Size (RSS) that exceed a
  minimum threshold
- Terminates these processes until enough memory is freed

This approach ensures that the maximum amount of memory is reclaimed with
minimal task termination. It's similar to Linux's OOM killer, though ours uses a
simpler heuristic based primarily on RSS.

**Swap Area Management**

We implemented a swap system using the provided AHCI disk driver, with the
second disk dedicated to swap operations. The swap area is structured as:

- A simple header at the beginning of disk containing "SWAP" magic bytes
- The rest of the 128MB disk divided into page-sized (4KB) logical blocks
- A `struct swap_info` that tracks available slots, used slots, and reference
  counts
- Linear mapping of swap entry IDs to disk locations

**Page Reclamation with LRU Algorithm**

To intelligently decide which pages to swap to disk, we implemented a two-list
LRU (Least Recently Used) algorithm:

- Active list: Contains recently accessed anonymous pages
- Inactive list: Contains pages that haven't been accessed recently
- Pages enter the active list when first allocated
- Periodically, we balance the lists by moving older active pages to inactive
- Pages at the tail of the inactive list are prime candidates for swapping

This approach ensures that we keep frequently used pages in memory while
swapping out those least likely to be needed soon.

**Reverse Mapping and Swapping**

A significant challenge was implementing reverse mapping - the ability to go
from a physical page to all virtual addresses mapping it. This was especially
complex with copy-on-write semantics from fork(). We implemented:

- `struct anon_vma` to track anonymous VMAs potentially mapping a page
- A red-black tree of `anon_vma_chain` structs for efficient lookups
- Virtual address tracking in the `page_info` struct

When swapping out a page, we:
1. Locate all VMAs that map the page
2. Mark the page table entries as not present but set a special PAGE_SWAPPED bit
3. Store the swap entry ID in the PTE
4. Write the page contents to disk
5. Free the physical page

When a process later accesses a swapped page, a page fault occurs, and we:
1. Check if the faulting address's PTE has the PAGE_SWAPPED bit
2. Retrieve the swap entry ID from the PTE
3. Allocate a new page
4. Read the page data from disk
5. Update the PTE to point to the new page

**Asynchronous I/O**

A critical requirement was that the kernel must never block while waiting for
disk operations. We implemented:

- Synchronous swapping for swap-in (page fault handling)
- Asynchronous swapping for swap-out (background reclamation)
- An I/O request queue with callbacks for handling completed operations
- A dedicated kernel thread to process this queue

For asynchronous operations, we created an `io_request` structure containing:
- Sector address, buffer pointer, and size
- Completion status tracking
- A callback function to execute when the I/O completes

## Performance Optimizations ("Bonuses")

We implemented two key optimizations for our swap system:

**Page Deduplication**

Our implementation efficiently tracks shared anonymous pages using a hierarchy
similar to Linux's `anon_vma` system. This provides:

- Efficient reverse mapping using a red-black tree for range lookups
- The ability to handle complex page-sharing scenarios during fork
- O(log n) lookup performance instead of O(n)

This optimization is particularly important for fork-heavy workloads, where many
processes might share the same physical pages through copy-on-write.

**Balanced Reclamation**

Our system balances between:

- Direct reclamation: Triggered immediately when memory is needed
- Periodic reclamation: A kernel thread (kswapd) that proactively swaps out
  pages before memory becomes critically low

This approach helps prevent situations where the system becomes unresponsive due
to memory pressure by maintaining a buffer of free pages at all times.

# The Complete Picture

With the swap system in place, our operating system could now:
- Handle memory-intensive workloads that exceed physical RAM
- Gracefully manage memory pressure through intelligent page reclamation
- Maintain system responsiveness even under heavy memory constraints
- Kill runaway memory hogs when absolutely necessary

This final component completed our journey from a bare-bones kernel to a fully
functional operating system with modern memory management capabilities.
Throughout this seven-lab course, we built every major subsystem of an operating
system from scratch - from boot code to multi-core scheduling.